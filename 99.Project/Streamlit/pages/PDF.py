import os
import streamlit as st
from dotenv import load_dotenv
from langchain_teddynote import logging
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_teddynote.prompts import load_prompt
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


load_dotenv()
logging.langsmith("[Project] PDF RAG")


# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
if not os.path.exists(".cache"):
    os.makedirs(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë” ì„¤ì •
if not os.path.exists(".cache/files"):
    os.makedirs(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.makedirs(".cache/embeddings")

st.title("ğŸ”¥PDF RAGğŸ”¥")

# ì²˜ìŒ í•œë²ˆë§Œ ì‹¤í–‰
if "messages" not in st.session_state:
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì§€ ì•Šì•˜ë‹¤ë©´ ì²´ì¸ ìƒì„± ì•ˆí•¨
    st.session_state["chain"] = None

# ì‚¬ì´ë“œ ë°”
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("Clear Chat", key="clear")

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("Upload a PDF", type="pdf")

    # ëª¨ë¸ ì„ íƒ ë©”ë‰´
    selected_model = st.selectbox("Select a model", ["gpt-4o-mini"])

# ì´ì „ ëŒ€í™” ì¶œë ¥
def print_messages():
    for chat_message in st.session_state.messages:
        st.chat_message(chat_message["role"]).markdown(chat_message["content"])

# ìƒˆë¡œìš´ ë©”ì‹œì§€ ì¶”ê°€
def add_message(role, content):
    st.session_state.messages.append({"role": role, "content": content})

# íŒŒì¼ì„ ìºì‹œì— ì €ì¥(ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ëŠ” ì‘ì—… ì²˜ë¦¬)
@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    # íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ì ì œê±°
    file_name = os.path.splitext(file.name)[0]
    embeddings_path = os.path.join(".cache/embeddings", f"{file_name}.faiss")
    embeddings = AzureOpenAIEmbeddings(
        azure_endpoint=os.environ["AZURE_OPENAI_EMBEDDINGS_ENDPOINT"],
        azure_deployment=os.environ["AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME"],
        api_key=os.environ["AZURE_OPENAI_EMBEDDINGS_API_KEY"],
    )
    
    # ì´ë¯¸ ì„ë² ë”©ëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
    if os.path.exists(embeddings_path):
        vectorstore = FAISS.load_local(embeddings_path, embeddings)
        return vectorstore.as_retriever()

    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ì²˜ë¦¬
    file_content = file.read()
    file_path = os.path.join(".cache/files", file.name)
    with open(file_path, "wb") as f:
        f.write(file_content)

    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ
    loader = PDFPlumberLoader(file_path)
    docs = loader.load()

    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_documents = text_splitter.split_documents(docs)

    # 3ë‹¨ê³„, 4ë‹¨ê³„: ì„ë² ë”©, ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

    # ì„ë² ë”© ê²°ê³¼ ì €ì¥
    vectorstore.save_local(embeddings_path)

    # 5ë‹¨ê³„: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
    retriever = vectorstore.as_retriever()
    
    return retriever

# ì²´ì¸ ìƒì„±
def create_chain(retriever, model_name="gpt-4o"):
    # 6ë‹¨ê³„: prompt ìƒì„±
    prompt = load_prompt("99.Project/Streamlit/prompts/pdf-rag.yaml", encoding="utf-8")

    # 7ë‹¨ê³„: LLM ìƒì„±
    llm = AzureChatOpenAI(
        azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
        azure_deployment=model_name,
        api_key=os.environ["AZURE_OPENAI_API_KEY"],
    )

    # 8ë‹¨ê³„: ì²´ì¸ ìƒì„±
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} 
        | prompt 
        | llm
        | StrOutputParser()
    )
    return chain
    
# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ
if uploaded_file:
    retriever = embed_file(uploaded_file)
    chain = create_chain(retriever, model_name=selected_model)
    st.session_state["chain"] = chain

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    # chain ìƒì„±
    chain = st.session_state["chain"]

    if chain is not None:
        # ì‚¬ìš©ì ì…ë ¥
        st.chat_message("user").write(user_input)

        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = chain.stream(user_input)
        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        warning_msg.warning("íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")
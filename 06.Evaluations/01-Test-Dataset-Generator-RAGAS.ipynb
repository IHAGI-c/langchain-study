{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 합성 테스트 데이터셋 생성\n",
    "\n",
    "**왜 합성 테스트 데이터(Synthetic Test Dataset) 인가?**\n",
    "\n",
    "RAG(검색 증강 생성) 증강 파이프라인의 성능을 평가하는 것은 매우 중요합니다. \n",
    "\n",
    "그러나 문서에서 수백 개의 QA(질문-문맥-응답) 샘플을 수동으로 생성하는 것은 시간과 노동력이 많이 소요될 수 있습니다. 또한 사람이 만든 질문은 철저한 평가에 필요한 복잡성 수준에 도달하기 어려워 궁극적으로 평가의 품질에 영향을 미칠 수 있습니다. \n",
    "\n",
    "합성 데이터 생성을 사용하면 데이터 집계 프로세스에서 **개발자의 시간을 90%** 까지 줄일 수 있습니다.\n",
    "\n",
    "- RAGAS: https://docs.ragas.io/en/latest/concepts/testset_generation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "06.Evaluations\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"06.Evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습에 활용한 문서\n",
    "\n",
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n",
    "\n",
    "_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서를 로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# 문서 로더 생성\n",
    "loader = PDFPlumberLoader(\"./data/AI 브리프 2023년 12월.pdf\")\n",
    "\n",
    "# 문서 로딩\n",
    "docs = loader.load()\n",
    "\n",
    "# 목차, 끝 페이지 제외\n",
    "docs = docs[3:-1]\n",
    "\n",
    "# 문서의 페이지수\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 문서 객체에는 `metadata` 를 통해 액세스할 수 있는 문서에 대한 추가 정보를 저장하는 데 사용할 수 있는 메타데이터 사전이 포함되어 있습니다. \n",
    "\n",
    "메타데이터 사전에는 `filename` 이라는 키가 포함되어 있는지 확인하세요. \n",
    "\n",
    "이 키는 Test datasets 생성 프로세스에서 활용될 것이므로. 메타데이터의 `filename` 속성은 동일한 문서에 속한 청크를 식별하는 데 사용됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/AI 브리프 2023년 12월.pdf',\n",
       " 'file_path': './data/AI 브리프 2023년 12월.pdf',\n",
       " 'page': 3,\n",
       " 'total_pages': 23,\n",
       " 'Author': 'dj',\n",
       " 'Creator': 'Hwp 2018 10.0.0.13462',\n",
       " 'Producer': 'Hancom PDF 1.3.0.542',\n",
       " 'CreationDate': \"D:20231208132838+09'00'\",\n",
       " 'ModDate': \"D:20231208132838+09'00'\",\n",
       " 'PDFVersion': '1.4'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata 설정(filename 이 존재해야 함)\n",
    "for doc in docs:\n",
    "    doc.metadata[\"filename\"] = doc.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i-esg-frontend/Library/Caches/pypoetry/virtualenvs/langchain-study-RiqbSEqC-py3.11/lib/python3.11/site-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "/Users/i-esg-frontend/Library/Caches/pypoetry/virtualenvs/langchain-study-RiqbSEqC-py3.11/lib/python3.11/site-packages/ragas/metrics/__init__.py:8: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.extractor import KeyphraseExtractor\n",
    "from ragas.testset.docstore import InMemoryDocumentStore\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# 데이터셋 생성기\n",
    "generator_llm = AzureChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# 데이터셋 비평기\n",
    "critic_llm = AzureChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# 문서 임베딩\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDINGS_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_EMBEDDINGS_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocumentStore를 초기화합니다. 사용자 정의 LLM과 임베딩을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할기를 설정합니다.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# LangChain의 ChatOpenAI 모델을 LangchainLLMWrapper로 감싸 Ragas와 호환되게 만듭니다.\n",
    "langchain_llm = LangchainLLMWrapper(AzureChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "# 주요 구문 추출기를 초기화합니다. 위에서 정의한 LLM을 사용합니다.\n",
    "keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)\n",
    "\n",
    "# ragas_embeddings 생성\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# InMemoryDocumentStore를 초기화합니다.\n",
    "# 이는 문서를 메모리에 저장하고 관리하는 저장소입니다.\n",
    "docstore = InMemoryDocumentStore(\n",
    "    splitter=splitter,\n",
    "    embeddings=ragas_embeddings,\n",
    "    extractor=keyphrase_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TestSet 을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    ragas_embeddings,\n",
    "    docstore=docstore,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**질문의 유형별 분포**\n",
    "\n",
    "- simple: 간단한 질문\n",
    "- reasoning: 추론이 필요한 질문\n",
    "- multi_context: 여러 맥락을 고려해야 하는 질문\n",
    "- conditional: 조건부 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 유형별 분포 결정\n",
    "# simple: 간단한 질문, reasoning: 추론이 필요한 질문, multi_context: 여러 맥락을 고려해야 하는 질문, conditional: 조건부 질문\n",
    "distributions = {simple: 0.4, reasoning: 0.2, multi_context: 0.2, conditional: 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- documents: 문서 데이터\n",
    "- test_size: 생성할 질문의 수\n",
    "- distributions: 질문 유형별 분포\n",
    "- with_debugging_logs: 디버깅 로그 출력 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e52b76c3cb2458ea837b2c3f18dfb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 테스트셋 생성\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# docs: 문서 데이터, 10: 생성할 질문의 수, distributions: 질문 유형별 분포, with_debugging_logs: 디버깅 로그 출력 여부\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-study-RiqbSEqC-py3.11/lib/python3.11/site-packages/ragas/testset/generator.py:206\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    204\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-study-RiqbSEqC-py3.11/lib/python3.11/site-packages/ragas/testset/docstore.py:214\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[0;34m(self, docs, show_progress)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[1;32m    210\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    211\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[1;32m    213\u001b[0m ]\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-study-RiqbSEqC-py3.11/lib/python3.11/site-packages/ragas/testset/docstore.py:253\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    251\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nodes_to_embed\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "# 테스트셋 생성\n",
    "# docs: 문서 데이터, 10: 생성할 질문의 수, distributions: 질문 유형별 분포, with_debugging_logs: 디버깅 로그 출력 여부\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    documents=docs,\n",
    "    test_size=10,\n",
    "    distributions=distributions,\n",
    "    with_debugging_logs=False,\n",
    "    raise_exceptions=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 생성된 테스트셋을 pandas DataFrame으로 변환\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mtestset\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      3\u001b[0m test_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testset' is not defined"
     ]
    }
   ],
   "source": [
    "# 생성된 테스트셋을 pandas DataFrame으로 변환\n",
    "test_df = testset.to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 에 저장된 데이터셋을 csv 파일로 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame의 상위 5개 행 출력\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 CSV 파일로 저장\n",
    "test_df.to_csv(\"data/ragas_synthetic_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
